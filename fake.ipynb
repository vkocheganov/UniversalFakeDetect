{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3693265/1276502182.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(opt.model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing progan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:48<00:00, 165.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sizes: [256.00+/-0.00] x [256.00+/-0.00] = [0.07+/-0.00 Mpix]\n",
      "Num reals: -68000, Num fakes: 76000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[8.68100480e-10 1.42288514e-10 1.03142118e-14 ... 1.00000000e+00\n 1.00000000e+00 1.00000000e+00].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m   \u001b[43minfer_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_to_infer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m infer_dir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/ssd4tb/vk/dollar/dollar_dataset/\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n",
      "Cell \u001b[0;32mIn[3], line 93\u001b[0m, in \u001b[0;36minfer_dir\u001b[0;34m(directory, model)\u001b[0m\n\u001b[1;32m     91\u001b[0m f_acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_true[y_true\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m], y_pred[y_true\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     92\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_true, y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m ap \u001b[38;5;241m=\u001b[39m \u001b[43maverage_precision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAP: \u001b[39m\u001b[38;5;132;01m{:2.2f}\u001b[39;00m\u001b[38;5;124m, Acc: \u001b[39m\u001b[38;5;132;01m{:2.2f}\u001b[39;00m\u001b[38;5;124m, Acc (real): \u001b[39m\u001b[38;5;132;01m{:2.2f}\u001b[39;00m\u001b[38;5;124m, Acc (fake): \u001b[39m\u001b[38;5;132;01m{:2.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ap\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100.\u001b[39m, acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100.\u001b[39m, r_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100.\u001b[39m, f_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100.\u001b[39m))\n",
      "File \u001b[0;32m~/work/miniconda3/envs/fakedet/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/work/miniconda3/envs/fakedet/lib/python3.12/site-packages/sklearn/metrics/_ranking.py:268\u001b[0m, in \u001b[0;36maverage_precision_score\u001b[0;34m(y_true, y_score, average, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    263\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mpresent_labels)\n\u001b[1;32m    265\u001b[0m average_precision \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    266\u001b[0m     _binary_uninterpolated_average_precision, pos_label\u001b[38;5;241m=\u001b[39mpos_label\n\u001b[1;32m    267\u001b[0m )\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/miniconda3/envs/fakedet/lib/python3.12/site-packages/sklearn/metrics/_base.py:80\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     78\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m     79\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n\u001b[0;32m---> 80\u001b[0m y_score \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m not_average_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     83\u001b[0m score_weight \u001b[38;5;241m=\u001b[39m sample_weight\n",
      "File \u001b[0;32m~/work/miniconda3/envs/fakedet/lib/python3.12/site-packages/sklearn/utils/validation.py:1050\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1045\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1046\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1047\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1048\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m             )\n\u001b[0;32m-> 1050\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[8.68100480e-10 1.42288514e-10 1.03142118e-14 ... 1.00000000e+00\n 1.00000000e+00 1.00000000e+00].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "base_fold = f\"/home/vkocheganov/work/research_projects/dollar/\"\n",
    "sys.path.insert(0, os.path.join(base_fold, \"CNNDetection\"))\n",
    "from networks.resnet import resnet50\n",
    "\n",
    "class OPT: pass\n",
    "opt = OPT()\n",
    "# opt.dir = os.path.join(base_fold, 'cnndet_datasets', 'CNN_synth_testset', 'deepfake')\n",
    "\n",
    "opt.model_path = os.path.join(base_fold, \"CNNDetection\", \"weights\", \"blur_jpg_prob0.1.pth\")\n",
    "opt.batch_size = 1\n",
    "opt.workers = 4\n",
    "opt.crop = None\n",
    "opt.use_cpu = False\n",
    "opt.size_only = False\n",
    "\n",
    "# Load model\n",
    "if(not opt.size_only):\n",
    "  model = resnet50(num_classes=1)\n",
    "  if(opt.model_path is not None):\n",
    "      state_dict = torch.load(opt.model_path, map_location='cpu')\n",
    "  model.load_state_dict(state_dict['model'])\n",
    "  model.eval()\n",
    "  if(not opt.use_cpu):\n",
    "      model.cuda()\n",
    "      \n",
    "import gc\n",
    "def infer_dir(directory, model):\n",
    "  gc.collect()\n",
    "  opt.dir = directory# os.path.join(\"/mnt/ssd4tb/vk/dollar/dollar_dataset/\")\n",
    "\n",
    "  # Transform\n",
    "  trans_init = []\n",
    "  if(opt.crop is not None):\n",
    "    trans_init = [transforms.CenterCrop(opt.crop),]\n",
    "    # print('Cropping to [%i]'%opt.crop)\n",
    "  # else:\n",
    "    # print('Not cropping')\n",
    "  trans = transforms.Compose(\n",
    "    trans_init + [\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),]\n",
    "    )\n",
    "\n",
    "  # Dataset loader\n",
    "  if(type(opt.dir)==str):\n",
    "    opt.dir = [opt.dir,]\n",
    "\n",
    "  # print('Loading [%i] datasets'%len(opt.dir))\n",
    "  data_loaders = []\n",
    "  for direct in opt.dir:\n",
    "    dataset = datasets.ImageFolder(direct, transform=trans)\n",
    "    data_loaders+=[torch.utils.data.DataLoader(dataset,\n",
    "                                            batch_size=opt.batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=opt.workers),]\n",
    "\n",
    "  y_true, y_pred = [], []\n",
    "  Hs, Ws = [], []\n",
    "  with torch.no_grad():\n",
    "    for data_loader in data_loaders:\n",
    "      for data, label in tqdm(data_loader[:100]):\n",
    "      # for data, label in data_loader:\n",
    "\n",
    "        Hs.append(data.shape[2])\n",
    "        Ws.append(data.shape[3])\n",
    "\n",
    "        y_true.extend(label.flatten().tolist())\n",
    "        if(not opt.size_only):\n",
    "          if(not opt.use_cpu):\n",
    "              data = data.cuda()\n",
    "          y_pred.extend(model(data).sigmoid().flatten().tolist())\n",
    "\n",
    "  Hs, Ws = np.array(Hs), np.array(Ws)\n",
    "  y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "  print('Average sizes: [{:2.2f}+/-{:2.2f}] x [{:2.2f}+/-{:2.2f}] = [{:2.2f}+/-{:2.2f} Mpix]'.format(np.mean(Hs), np.std(Hs), np.mean(Ws), np.std(Ws), np.mean(Hs*Ws)/1e6, np.std(Hs*Ws)/1e6))\n",
    "  print('Num reals: {}, Num fakes: {}'.format(np.sum(1-y_true), np.sum(y_true)))\n",
    "\n",
    "  if(not opt.size_only):\n",
    "    r_acc = accuracy_score(y_true[y_true==0], y_pred[y_true==0] > 0.5)\n",
    "    f_acc = accuracy_score(y_true[y_true==1], y_pred[y_true==1] > 0.5)\n",
    "    acc = accuracy_score(y_true, y_pred > 0.5)\n",
    "    print(type(y_true), y_true)\n",
    "    print(type(y_pred), y_pred)\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    print('AP: {:2.2f}, Acc: {:2.2f}, Acc (real): {:2.2f}, Acc (fake): {:2.2f}'.format(ap*100., acc*100., r_acc*100., f_acc*100.))\n",
    "\n",
    "dir_to_infer = \"/home/vkocheganov/work/research_projects/dollar/cnndet_datasets/CNN_synth_testset/\"\n",
    "for curr_dir in os.listdir(dir_to_infer):\n",
    "  if curr_dir != \"progan\":\n",
    "    continue\n",
    "  print(f\"processing {curr_dir}\")\n",
    "  infer_dir(os.path.join(dir_to_infer, curr_dir), model)\n",
    "\n",
    "print(f\"processing test\")\n",
    "infer_dir(\"/mnt/ssd4tb/vk/dollar/dollar_dataset/\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /tmp/ipykernel_3693265/638286014.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
    "#   state_dict = torch.load(opt.model_path, map_location='cpu')\n",
    "# processing gaugan\n",
    "# 100%|██████████| 10000/10000 [01:01<00:00, 162.45it/s]\n",
    "# Average sizes: [256.00+/-0.00] x [256.00+/-0.00] = [0.07+/-0.00 Mpix]\n",
    "# Num reals: 5000, Num fakes: 5000\n",
    "# AP: 90.80, Acc: 81.42, Acc (real): 92.98, Acc (fake): 69.86\n",
    "# processing whichfaceisreal\n",
    "# 100%|██████████| 2000/2000 [01:07<00:00, 29.82it/s]\n",
    "# Average sizes: [1024.00+/-0.00] x [1024.00+/-0.00] = [1.05+/-0.00 Mpix]\n",
    "# Num reals: 1000, Num fakes: 1000\n",
    "# AP: 99.91, Acc: 95.10, Acc (real): 99.80, Acc (fake): 90.40\n",
    "# processing seeingdark\n",
    "# 100%|██████████| 360/360 [02:56<00:00,  2.04it/s]\n",
    "# Average sizes: [3420.27+/-591.67] x [5113.43+/-886.51] = [18.01+/-6.09 Mpix]\n",
    "# Num reals: 180, Num fakes: 180\n",
    "# AP: 99.81, Acc: 98.06, Acc (real): 97.78, Acc (fake): 98.33\n",
    "# processing crn\n",
    "# 100%|██████████| 12764/12764 [01:29<00:00, 143.19it/s]\n",
    "# Average sizes: [256.00+/-0.00] x [512.00+/-0.00] = [0.13+/-0.00 Mpix]\n",
    "# Num reals: 6382, Num fakes: 6382\n",
    "# AP: 99.84, Acc: 86.35, Acc (real): 72.70, Acc (fake): 100.00\n",
    "# processing san\n",
    "# 100%|██████████| 438/438 [00:07<00:00, 56.96it/s] \n",
    "# Average sizes: [563.13+/-244.18] x [690.60+/-287.70] = [0.44+/-0.32 Mpix]\n",
    "# Num reals: 219, Num fakes: 219\n",
    "# AP: 68.57, Acc: 50.00, Acc (real): 99.54, Acc (fake): 0.46\n",
    "# processing progan\n",
    "# 100%|██████████| 8000/8000 [00:49<00:00, 161.28it/s]\n",
    "# Average sizes: [256.00+/-0.00] x [256.00+/-0.00] = [0.07+/-0.00 Mpix]\n",
    "# Num reals: -68000, Num fakes: 76000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakedet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
