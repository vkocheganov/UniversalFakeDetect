{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3195792/2099756496.py:25: DeprecationWarning: Please import `gaussian_filter` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  from scipy.ndimage.filters import gaussian_filter\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_fold = f\"/home/vkocheganov/work/research_projects/dollar/\"\n",
    "sys.path.insert(0, os.path.join(base_fold, \"UniversalFakeDetect\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import argparse\n",
    "from ast import arg\n",
    "import csv\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "import sys\n",
    "from models import get_model\n",
    "from PIL import Image \n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from copy import deepcopy\n",
    "from dataset_paths_ipy import DATASET_PATHS\n",
    "import random\n",
    "import shutil\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3195792/447730165.py:256: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(opt.ckpt, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['model.positional_embedding', 'model.text_projection', 'model.logit_scale', 'model.visual.class_embedding', 'model.visual.positional_embedding', 'model.visual.proj', 'model.visual.conv1.weight', 'model.visual.ln_pre.weight', 'model.visual.ln_pre.bias', 'model.visual.transformer.resblocks.0.attn.in_proj_weight', 'model.visual.transformer.resblocks.0.attn.in_proj_bias', 'model.visual.transformer.resblocks.0.attn.out_proj.weight', 'model.visual.transformer.resblocks.0.attn.out_proj.bias', 'model.visual.transformer.resblocks.0.ln_1.weight', 'model.visual.transformer.resblocks.0.ln_1.bias', 'model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'model.visual.transformer.resblocks.0.ln_2.weight', 'model.visual.transformer.resblocks.0.ln_2.bias', 'model.visual.transformer.resblocks.1.attn.in_proj_weight', 'model.visual.transformer.resblocks.1.attn.in_proj_bias', 'model.visual.transformer.resblocks.1.attn.out_proj.weight', 'model.visual.transformer.resblocks.1.attn.out_proj.bias', 'model.visual.transformer.resblocks.1.ln_1.weight', 'model.visual.transformer.resblocks.1.ln_1.bias', 'model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'model.visual.transformer.resblocks.1.ln_2.weight', 'model.visual.transformer.resblocks.1.ln_2.bias', 'model.visual.transformer.resblocks.2.attn.in_proj_weight', 'model.visual.transformer.resblocks.2.attn.in_proj_bias', 'model.visual.transformer.resblocks.2.attn.out_proj.weight', 'model.visual.transformer.resblocks.2.attn.out_proj.bias', 'model.visual.transformer.resblocks.2.ln_1.weight', 'model.visual.transformer.resblocks.2.ln_1.bias', 'model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'model.visual.transformer.resblocks.2.ln_2.weight', 'model.visual.transformer.resblocks.2.ln_2.bias', 'model.visual.transformer.resblocks.3.attn.in_proj_weight', 'model.visual.transformer.resblocks.3.attn.in_proj_bias', 'model.visual.transformer.resblocks.3.attn.out_proj.weight', 'model.visual.transformer.resblocks.3.attn.out_proj.bias', 'model.visual.transformer.resblocks.3.ln_1.weight', 'model.visual.transformer.resblocks.3.ln_1.bias', 'model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'model.visual.transformer.resblocks.3.ln_2.weight', 'model.visual.transformer.resblocks.3.ln_2.bias', 'model.visual.transformer.resblocks.4.attn.in_proj_weight', 'model.visual.transformer.resblocks.4.attn.in_proj_bias', 'model.visual.transformer.resblocks.4.attn.out_proj.weight', 'model.visual.transformer.resblocks.4.attn.out_proj.bias', 'model.visual.transformer.resblocks.4.ln_1.weight', 'model.visual.transformer.resblocks.4.ln_1.bias', 'model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'model.visual.transformer.resblocks.4.ln_2.weight', 'model.visual.transformer.resblocks.4.ln_2.bias', 'model.visual.transformer.resblocks.5.attn.in_proj_weight', 'model.visual.transformer.resblocks.5.attn.in_proj_bias', 'model.visual.transformer.resblocks.5.attn.out_proj.weight', 'model.visual.transformer.resblocks.5.attn.out_proj.bias', 'model.visual.transformer.resblocks.5.ln_1.weight', 'model.visual.transformer.resblocks.5.ln_1.bias', 'model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'model.visual.transformer.resblocks.5.ln_2.weight', 'model.visual.transformer.resblocks.5.ln_2.bias', 'model.visual.transformer.resblocks.6.attn.in_proj_weight', 'model.visual.transformer.resblocks.6.attn.in_proj_bias', 'model.visual.transformer.resblocks.6.attn.out_proj.weight', 'model.visual.transformer.resblocks.6.attn.out_proj.bias', 'model.visual.transformer.resblocks.6.ln_1.weight', 'model.visual.transformer.resblocks.6.ln_1.bias', 'model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'model.visual.transformer.resblocks.6.ln_2.weight', 'model.visual.transformer.resblocks.6.ln_2.bias', 'model.visual.transformer.resblocks.7.attn.in_proj_weight', 'model.visual.transformer.resblocks.7.attn.in_proj_bias', 'model.visual.transformer.resblocks.7.attn.out_proj.weight', 'model.visual.transformer.resblocks.7.attn.out_proj.bias', 'model.visual.transformer.resblocks.7.ln_1.weight', 'model.visual.transformer.resblocks.7.ln_1.bias', 'model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'model.visual.transformer.resblocks.7.ln_2.weight', 'model.visual.transformer.resblocks.7.ln_2.bias', 'model.visual.transformer.resblocks.8.attn.in_proj_weight', 'model.visual.transformer.resblocks.8.attn.in_proj_bias', 'model.visual.transformer.resblocks.8.attn.out_proj.weight', 'model.visual.transformer.resblocks.8.attn.out_proj.bias', 'model.visual.transformer.resblocks.8.ln_1.weight', 'model.visual.transformer.resblocks.8.ln_1.bias', 'model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'model.visual.transformer.resblocks.8.ln_2.weight', 'model.visual.transformer.resblocks.8.ln_2.bias', 'model.visual.transformer.resblocks.9.attn.in_proj_weight', 'model.visual.transformer.resblocks.9.attn.in_proj_bias', 'model.visual.transformer.resblocks.9.attn.out_proj.weight', 'model.visual.transformer.resblocks.9.attn.out_proj.bias', 'model.visual.transformer.resblocks.9.ln_1.weight', 'model.visual.transformer.resblocks.9.ln_1.bias', 'model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'model.visual.transformer.resblocks.9.ln_2.weight', 'model.visual.transformer.resblocks.9.ln_2.bias', 'model.visual.transformer.resblocks.10.attn.in_proj_weight', 'model.visual.transformer.resblocks.10.attn.in_proj_bias', 'model.visual.transformer.resblocks.10.attn.out_proj.weight', 'model.visual.transformer.resblocks.10.attn.out_proj.bias', 'model.visual.transformer.resblocks.10.ln_1.weight', 'model.visual.transformer.resblocks.10.ln_1.bias', 'model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'model.visual.transformer.resblocks.10.ln_2.weight', 'model.visual.transformer.resblocks.10.ln_2.bias', 'model.visual.transformer.resblocks.11.attn.in_proj_weight', 'model.visual.transformer.resblocks.11.attn.in_proj_bias', 'model.visual.transformer.resblocks.11.attn.out_proj.weight', 'model.visual.transformer.resblocks.11.attn.out_proj.bias', 'model.visual.transformer.resblocks.11.ln_1.weight', 'model.visual.transformer.resblocks.11.ln_1.bias', 'model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'model.visual.transformer.resblocks.11.ln_2.weight', 'model.visual.transformer.resblocks.11.ln_2.bias', 'model.visual.transformer.resblocks.12.attn.in_proj_weight', 'model.visual.transformer.resblocks.12.attn.in_proj_bias', 'model.visual.transformer.resblocks.12.attn.out_proj.weight', 'model.visual.transformer.resblocks.12.attn.out_proj.bias', 'model.visual.transformer.resblocks.12.ln_1.weight', 'model.visual.transformer.resblocks.12.ln_1.bias', 'model.visual.transformer.resblocks.12.mlp.c_fc.weight', 'model.visual.transformer.resblocks.12.mlp.c_fc.bias', 'model.visual.transformer.resblocks.12.mlp.c_proj.weight', 'model.visual.transformer.resblocks.12.mlp.c_proj.bias', 'model.visual.transformer.resblocks.12.ln_2.weight', 'model.visual.transformer.resblocks.12.ln_2.bias', 'model.visual.transformer.resblocks.13.attn.in_proj_weight', 'model.visual.transformer.resblocks.13.attn.in_proj_bias', 'model.visual.transformer.resblocks.13.attn.out_proj.weight', 'model.visual.transformer.resblocks.13.attn.out_proj.bias', 'model.visual.transformer.resblocks.13.ln_1.weight', 'model.visual.transformer.resblocks.13.ln_1.bias', 'model.visual.transformer.resblocks.13.mlp.c_fc.weight', 'model.visual.transformer.resblocks.13.mlp.c_fc.bias', 'model.visual.transformer.resblocks.13.mlp.c_proj.weight', 'model.visual.transformer.resblocks.13.mlp.c_proj.bias', 'model.visual.transformer.resblocks.13.ln_2.weight', 'model.visual.transformer.resblocks.13.ln_2.bias', 'model.visual.transformer.resblocks.14.attn.in_proj_weight', 'model.visual.transformer.resblocks.14.attn.in_proj_bias', 'model.visual.transformer.resblocks.14.attn.out_proj.weight', 'model.visual.transformer.resblocks.14.attn.out_proj.bias', 'model.visual.transformer.resblocks.14.ln_1.weight', 'model.visual.transformer.resblocks.14.ln_1.bias', 'model.visual.transformer.resblocks.14.mlp.c_fc.weight', 'model.visual.transformer.resblocks.14.mlp.c_fc.bias', 'model.visual.transformer.resblocks.14.mlp.c_proj.weight', 'model.visual.transformer.resblocks.14.mlp.c_proj.bias', 'model.visual.transformer.resblocks.14.ln_2.weight', 'model.visual.transformer.resblocks.14.ln_2.bias', 'model.visual.transformer.resblocks.15.attn.in_proj_weight', 'model.visual.transformer.resblocks.15.attn.in_proj_bias', 'model.visual.transformer.resblocks.15.attn.out_proj.weight', 'model.visual.transformer.resblocks.15.attn.out_proj.bias', 'model.visual.transformer.resblocks.15.ln_1.weight', 'model.visual.transformer.resblocks.15.ln_1.bias', 'model.visual.transformer.resblocks.15.mlp.c_fc.weight', 'model.visual.transformer.resblocks.15.mlp.c_fc.bias', 'model.visual.transformer.resblocks.15.mlp.c_proj.weight', 'model.visual.transformer.resblocks.15.mlp.c_proj.bias', 'model.visual.transformer.resblocks.15.ln_2.weight', 'model.visual.transformer.resblocks.15.ln_2.bias', 'model.visual.transformer.resblocks.16.attn.in_proj_weight', 'model.visual.transformer.resblocks.16.attn.in_proj_bias', 'model.visual.transformer.resblocks.16.attn.out_proj.weight', 'model.visual.transformer.resblocks.16.attn.out_proj.bias', 'model.visual.transformer.resblocks.16.ln_1.weight', 'model.visual.transformer.resblocks.16.ln_1.bias', 'model.visual.transformer.resblocks.16.mlp.c_fc.weight', 'model.visual.transformer.resblocks.16.mlp.c_fc.bias', 'model.visual.transformer.resblocks.16.mlp.c_proj.weight', 'model.visual.transformer.resblocks.16.mlp.c_proj.bias', 'model.visual.transformer.resblocks.16.ln_2.weight', 'model.visual.transformer.resblocks.16.ln_2.bias', 'model.visual.transformer.resblocks.17.attn.in_proj_weight', 'model.visual.transformer.resblocks.17.attn.in_proj_bias', 'model.visual.transformer.resblocks.17.attn.out_proj.weight', 'model.visual.transformer.resblocks.17.attn.out_proj.bias', 'model.visual.transformer.resblocks.17.ln_1.weight', 'model.visual.transformer.resblocks.17.ln_1.bias', 'model.visual.transformer.resblocks.17.mlp.c_fc.weight', 'model.visual.transformer.resblocks.17.mlp.c_fc.bias', 'model.visual.transformer.resblocks.17.mlp.c_proj.weight', 'model.visual.transformer.resblocks.17.mlp.c_proj.bias', 'model.visual.transformer.resblocks.17.ln_2.weight', 'model.visual.transformer.resblocks.17.ln_2.bias', 'model.visual.transformer.resblocks.18.attn.in_proj_weight', 'model.visual.transformer.resblocks.18.attn.in_proj_bias', 'model.visual.transformer.resblocks.18.attn.out_proj.weight', 'model.visual.transformer.resblocks.18.attn.out_proj.bias', 'model.visual.transformer.resblocks.18.ln_1.weight', 'model.visual.transformer.resblocks.18.ln_1.bias', 'model.visual.transformer.resblocks.18.mlp.c_fc.weight', 'model.visual.transformer.resblocks.18.mlp.c_fc.bias', 'model.visual.transformer.resblocks.18.mlp.c_proj.weight', 'model.visual.transformer.resblocks.18.mlp.c_proj.bias', 'model.visual.transformer.resblocks.18.ln_2.weight', 'model.visual.transformer.resblocks.18.ln_2.bias', 'model.visual.transformer.resblocks.19.attn.in_proj_weight', 'model.visual.transformer.resblocks.19.attn.in_proj_bias', 'model.visual.transformer.resblocks.19.attn.out_proj.weight', 'model.visual.transformer.resblocks.19.attn.out_proj.bias', 'model.visual.transformer.resblocks.19.ln_1.weight', 'model.visual.transformer.resblocks.19.ln_1.bias', 'model.visual.transformer.resblocks.19.mlp.c_fc.weight', 'model.visual.transformer.resblocks.19.mlp.c_fc.bias', 'model.visual.transformer.resblocks.19.mlp.c_proj.weight', 'model.visual.transformer.resblocks.19.mlp.c_proj.bias', 'model.visual.transformer.resblocks.19.ln_2.weight', 'model.visual.transformer.resblocks.19.ln_2.bias', 'model.visual.transformer.resblocks.20.attn.in_proj_weight', 'model.visual.transformer.resblocks.20.attn.in_proj_bias', 'model.visual.transformer.resblocks.20.attn.out_proj.weight', 'model.visual.transformer.resblocks.20.attn.out_proj.bias', 'model.visual.transformer.resblocks.20.ln_1.weight', 'model.visual.transformer.resblocks.20.ln_1.bias', 'model.visual.transformer.resblocks.20.mlp.c_fc.weight', 'model.visual.transformer.resblocks.20.mlp.c_fc.bias', 'model.visual.transformer.resblocks.20.mlp.c_proj.weight', 'model.visual.transformer.resblocks.20.mlp.c_proj.bias', 'model.visual.transformer.resblocks.20.ln_2.weight', 'model.visual.transformer.resblocks.20.ln_2.bias', 'model.visual.transformer.resblocks.21.attn.in_proj_weight', 'model.visual.transformer.resblocks.21.attn.in_proj_bias', 'model.visual.transformer.resblocks.21.attn.out_proj.weight', 'model.visual.transformer.resblocks.21.attn.out_proj.bias', 'model.visual.transformer.resblocks.21.ln_1.weight', 'model.visual.transformer.resblocks.21.ln_1.bias', 'model.visual.transformer.resblocks.21.mlp.c_fc.weight', 'model.visual.transformer.resblocks.21.mlp.c_fc.bias', 'model.visual.transformer.resblocks.21.mlp.c_proj.weight', 'model.visual.transformer.resblocks.21.mlp.c_proj.bias', 'model.visual.transformer.resblocks.21.ln_2.weight', 'model.visual.transformer.resblocks.21.ln_2.bias', 'model.visual.transformer.resblocks.22.attn.in_proj_weight', 'model.visual.transformer.resblocks.22.attn.in_proj_bias', 'model.visual.transformer.resblocks.22.attn.out_proj.weight', 'model.visual.transformer.resblocks.22.attn.out_proj.bias', 'model.visual.transformer.resblocks.22.ln_1.weight', 'model.visual.transformer.resblocks.22.ln_1.bias', 'model.visual.transformer.resblocks.22.mlp.c_fc.weight', 'model.visual.transformer.resblocks.22.mlp.c_fc.bias', 'model.visual.transformer.resblocks.22.mlp.c_proj.weight', 'model.visual.transformer.resblocks.22.mlp.c_proj.bias', 'model.visual.transformer.resblocks.22.ln_2.weight', 'model.visual.transformer.resblocks.22.ln_2.bias', 'model.visual.transformer.resblocks.23.attn.in_proj_weight', 'model.visual.transformer.resblocks.23.attn.in_proj_bias', 'model.visual.transformer.resblocks.23.attn.out_proj.weight', 'model.visual.transformer.resblocks.23.attn.out_proj.bias', 'model.visual.transformer.resblocks.23.ln_1.weight', 'model.visual.transformer.resblocks.23.ln_1.bias', 'model.visual.transformer.resblocks.23.mlp.c_fc.weight', 'model.visual.transformer.resblocks.23.mlp.c_fc.bias', 'model.visual.transformer.resblocks.23.mlp.c_proj.weight', 'model.visual.transformer.resblocks.23.mlp.c_proj.bias', 'model.visual.transformer.resblocks.23.ln_2.weight', 'model.visual.transformer.resblocks.23.ln_2.bias', 'model.visual.ln_post.weight', 'model.visual.ln_post.bias', 'model.transformer.resblocks.0.attn.in_proj_weight', 'model.transformer.resblocks.0.attn.in_proj_bias', 'model.transformer.resblocks.0.attn.out_proj.weight', 'model.transformer.resblocks.0.attn.out_proj.bias', 'model.transformer.resblocks.0.ln_1.weight', 'model.transformer.resblocks.0.ln_1.bias', 'model.transformer.resblocks.0.mlp.c_fc.weight', 'model.transformer.resblocks.0.mlp.c_fc.bias', 'model.transformer.resblocks.0.mlp.c_proj.weight', 'model.transformer.resblocks.0.mlp.c_proj.bias', 'model.transformer.resblocks.0.ln_2.weight', 'model.transformer.resblocks.0.ln_2.bias', 'model.transformer.resblocks.1.attn.in_proj_weight', 'model.transformer.resblocks.1.attn.in_proj_bias', 'model.transformer.resblocks.1.attn.out_proj.weight', 'model.transformer.resblocks.1.attn.out_proj.bias', 'model.transformer.resblocks.1.ln_1.weight', 'model.transformer.resblocks.1.ln_1.bias', 'model.transformer.resblocks.1.mlp.c_fc.weight', 'model.transformer.resblocks.1.mlp.c_fc.bias', 'model.transformer.resblocks.1.mlp.c_proj.weight', 'model.transformer.resblocks.1.mlp.c_proj.bias', 'model.transformer.resblocks.1.ln_2.weight', 'model.transformer.resblocks.1.ln_2.bias', 'model.transformer.resblocks.2.attn.in_proj_weight', 'model.transformer.resblocks.2.attn.in_proj_bias', 'model.transformer.resblocks.2.attn.out_proj.weight', 'model.transformer.resblocks.2.attn.out_proj.bias', 'model.transformer.resblocks.2.ln_1.weight', 'model.transformer.resblocks.2.ln_1.bias', 'model.transformer.resblocks.2.mlp.c_fc.weight', 'model.transformer.resblocks.2.mlp.c_fc.bias', 'model.transformer.resblocks.2.mlp.c_proj.weight', 'model.transformer.resblocks.2.mlp.c_proj.bias', 'model.transformer.resblocks.2.ln_2.weight', 'model.transformer.resblocks.2.ln_2.bias', 'model.transformer.resblocks.3.attn.in_proj_weight', 'model.transformer.resblocks.3.attn.in_proj_bias', 'model.transformer.resblocks.3.attn.out_proj.weight', 'model.transformer.resblocks.3.attn.out_proj.bias', 'model.transformer.resblocks.3.ln_1.weight', 'model.transformer.resblocks.3.ln_1.bias', 'model.transformer.resblocks.3.mlp.c_fc.weight', 'model.transformer.resblocks.3.mlp.c_fc.bias', 'model.transformer.resblocks.3.mlp.c_proj.weight', 'model.transformer.resblocks.3.mlp.c_proj.bias', 'model.transformer.resblocks.3.ln_2.weight', 'model.transformer.resblocks.3.ln_2.bias', 'model.transformer.resblocks.4.attn.in_proj_weight', 'model.transformer.resblocks.4.attn.in_proj_bias', 'model.transformer.resblocks.4.attn.out_proj.weight', 'model.transformer.resblocks.4.attn.out_proj.bias', 'model.transformer.resblocks.4.ln_1.weight', 'model.transformer.resblocks.4.ln_1.bias', 'model.transformer.resblocks.4.mlp.c_fc.weight', 'model.transformer.resblocks.4.mlp.c_fc.bias', 'model.transformer.resblocks.4.mlp.c_proj.weight', 'model.transformer.resblocks.4.mlp.c_proj.bias', 'model.transformer.resblocks.4.ln_2.weight', 'model.transformer.resblocks.4.ln_2.bias', 'model.transformer.resblocks.5.attn.in_proj_weight', 'model.transformer.resblocks.5.attn.in_proj_bias', 'model.transformer.resblocks.5.attn.out_proj.weight', 'model.transformer.resblocks.5.attn.out_proj.bias', 'model.transformer.resblocks.5.ln_1.weight', 'model.transformer.resblocks.5.ln_1.bias', 'model.transformer.resblocks.5.mlp.c_fc.weight', 'model.transformer.resblocks.5.mlp.c_fc.bias', 'model.transformer.resblocks.5.mlp.c_proj.weight', 'model.transformer.resblocks.5.mlp.c_proj.bias', 'model.transformer.resblocks.5.ln_2.weight', 'model.transformer.resblocks.5.ln_2.bias', 'model.transformer.resblocks.6.attn.in_proj_weight', 'model.transformer.resblocks.6.attn.in_proj_bias', 'model.transformer.resblocks.6.attn.out_proj.weight', 'model.transformer.resblocks.6.attn.out_proj.bias', 'model.transformer.resblocks.6.ln_1.weight', 'model.transformer.resblocks.6.ln_1.bias', 'model.transformer.resblocks.6.mlp.c_fc.weight', 'model.transformer.resblocks.6.mlp.c_fc.bias', 'model.transformer.resblocks.6.mlp.c_proj.weight', 'model.transformer.resblocks.6.mlp.c_proj.bias', 'model.transformer.resblocks.6.ln_2.weight', 'model.transformer.resblocks.6.ln_2.bias', 'model.transformer.resblocks.7.attn.in_proj_weight', 'model.transformer.resblocks.7.attn.in_proj_bias', 'model.transformer.resblocks.7.attn.out_proj.weight', 'model.transformer.resblocks.7.attn.out_proj.bias', 'model.transformer.resblocks.7.ln_1.weight', 'model.transformer.resblocks.7.ln_1.bias', 'model.transformer.resblocks.7.mlp.c_fc.weight', 'model.transformer.resblocks.7.mlp.c_fc.bias', 'model.transformer.resblocks.7.mlp.c_proj.weight', 'model.transformer.resblocks.7.mlp.c_proj.bias', 'model.transformer.resblocks.7.ln_2.weight', 'model.transformer.resblocks.7.ln_2.bias', 'model.transformer.resblocks.8.attn.in_proj_weight', 'model.transformer.resblocks.8.attn.in_proj_bias', 'model.transformer.resblocks.8.attn.out_proj.weight', 'model.transformer.resblocks.8.attn.out_proj.bias', 'model.transformer.resblocks.8.ln_1.weight', 'model.transformer.resblocks.8.ln_1.bias', 'model.transformer.resblocks.8.mlp.c_fc.weight', 'model.transformer.resblocks.8.mlp.c_fc.bias', 'model.transformer.resblocks.8.mlp.c_proj.weight', 'model.transformer.resblocks.8.mlp.c_proj.bias', 'model.transformer.resblocks.8.ln_2.weight', 'model.transformer.resblocks.8.ln_2.bias', 'model.transformer.resblocks.9.attn.in_proj_weight', 'model.transformer.resblocks.9.attn.in_proj_bias', 'model.transformer.resblocks.9.attn.out_proj.weight', 'model.transformer.resblocks.9.attn.out_proj.bias', 'model.transformer.resblocks.9.ln_1.weight', 'model.transformer.resblocks.9.ln_1.bias', 'model.transformer.resblocks.9.mlp.c_fc.weight', 'model.transformer.resblocks.9.mlp.c_fc.bias', 'model.transformer.resblocks.9.mlp.c_proj.weight', 'model.transformer.resblocks.9.mlp.c_proj.bias', 'model.transformer.resblocks.9.ln_2.weight', 'model.transformer.resblocks.9.ln_2.bias', 'model.transformer.resblocks.10.attn.in_proj_weight', 'model.transformer.resblocks.10.attn.in_proj_bias', 'model.transformer.resblocks.10.attn.out_proj.weight', 'model.transformer.resblocks.10.attn.out_proj.bias', 'model.transformer.resblocks.10.ln_1.weight', 'model.transformer.resblocks.10.ln_1.bias', 'model.transformer.resblocks.10.mlp.c_fc.weight', 'model.transformer.resblocks.10.mlp.c_fc.bias', 'model.transformer.resblocks.10.mlp.c_proj.weight', 'model.transformer.resblocks.10.mlp.c_proj.bias', 'model.transformer.resblocks.10.ln_2.weight', 'model.transformer.resblocks.10.ln_2.bias', 'model.transformer.resblocks.11.attn.in_proj_weight', 'model.transformer.resblocks.11.attn.in_proj_bias', 'model.transformer.resblocks.11.attn.out_proj.weight', 'model.transformer.resblocks.11.attn.out_proj.bias', 'model.transformer.resblocks.11.ln_1.weight', 'model.transformer.resblocks.11.ln_1.bias', 'model.transformer.resblocks.11.mlp.c_fc.weight', 'model.transformer.resblocks.11.mlp.c_fc.bias', 'model.transformer.resblocks.11.mlp.c_proj.weight', 'model.transformer.resblocks.11.mlp.c_proj.bias', 'model.transformer.resblocks.11.ln_2.weight', 'model.transformer.resblocks.11.ln_2.bias', 'model.token_embedding.weight', 'model.ln_final.weight', 'model.ln_final.bias', 'fc.weight', 'fc.bias'])\n",
      "Model loaded..\n",
      "vic dataset_paths = [{'real_path': '/mnt/ssd4tb/vk/dollar/dollar_dataset', 'fake_path': '/mnt/ssd4tb/vk/dollar/dollar_dataset', 'data_mode': 'wang2020', 'key': 'dollar'}]\n",
      "processing dollar\n",
      "not enough images, max_sample falling to 100\n",
      "Length of dataset: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SEED = 0\n",
    "def set_seed():\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "\n",
    "MEAN = {\n",
    "    \"imagenet\":[0.485, 0.456, 0.406],\n",
    "    \"clip\":[0.48145466, 0.4578275, 0.40821073]\n",
    "}\n",
    "\n",
    "STD = {\n",
    "    \"imagenet\":[0.229, 0.224, 0.225],\n",
    "    \"clip\":[0.26862954, 0.26130258, 0.27577711]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_best_threshold(y_true, y_pred):\n",
    "    \"We assume first half is real 0, and the second half is fake 1\"\n",
    "\n",
    "    N = y_true.shape[0]\n",
    "\n",
    "    if y_pred[0:N//2].max() <= y_pred[N//2:N].min(): # perfectly separable case\n",
    "        return (y_pred[0:N//2].max() + y_pred[N//2:N].min()) / 2 \n",
    "\n",
    "    best_acc = 0 \n",
    "    best_thres = 0 \n",
    "    for thres in y_pred:\n",
    "        temp = deepcopy(y_pred)\n",
    "        temp[temp>=thres] = 1 \n",
    "        temp[temp<thres] = 0 \n",
    "\n",
    "        acc = (temp == y_true).sum() / N  \n",
    "        if acc >= best_acc:\n",
    "            best_thres = thres\n",
    "            best_acc = acc \n",
    "    \n",
    "    return best_thres\n",
    "        \n",
    "\n",
    " \n",
    "def png2jpg(img, quality):\n",
    "    out = BytesIO()\n",
    "    img.save(out, format='jpeg', quality=quality) # ranging from 0-95, 75 is default\n",
    "    img = Image.open(out)\n",
    "    # load from memory before ByteIO closes\n",
    "    img = np.array(img)\n",
    "    out.close()\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    img = np.array(img)\n",
    "\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_acc(y_true, y_pred, thres):\n",
    "    r_acc = accuracy_score(y_true[y_true==0], y_pred[y_true==0] > thres)\n",
    "    f_acc = accuracy_score(y_true[y_true==1], y_pred[y_true==1] > thres)\n",
    "    acc = accuracy_score(y_true, y_pred > thres)\n",
    "    return r_acc, f_acc, acc    \n",
    "\n",
    "\n",
    "def validate(model, loader, find_thres=False):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_true, y_pred = [], []\n",
    "        print (\"Length of dataset: %d\" %(len(loader)))\n",
    "        for img, label in loader:\n",
    "            in_tens = img.cuda()\n",
    "\n",
    "            y_pred.extend(model(in_tens).sigmoid().flatten().tolist())\n",
    "            y_true.extend(label.flatten().tolist())\n",
    "\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    # ================== save this if you want to plot the curves =========== # \n",
    "    # torch.save( torch.stack( [torch.tensor(y_true), torch.tensor(y_pred)] ),  'baseline_predication_for_pr_roc_curve.pth' )\n",
    "    # exit()\n",
    "    # =================================================================== #\n",
    "    \n",
    "    # Get AP \n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    # Acc based on 0.5\n",
    "    r_acc0, f_acc0, acc0 = calculate_acc(y_true, y_pred, 0.5)\n",
    "    if not find_thres:\n",
    "        return ap, r_acc0, f_acc0, acc0\n",
    "\n",
    "\n",
    "    # Acc based on the best thres\n",
    "    best_thres = find_best_threshold(y_true, y_pred)\n",
    "    r_acc1, f_acc1, acc1 = calculate_acc(y_true, y_pred, best_thres)\n",
    "\n",
    "    return ap, r_acc0, f_acc0, acc0, r_acc1, f_acc1, acc1, best_thres\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = # \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def recursively_read(rootdir, must_contain, exts=[\"png\", \"jpg\", \"JPEG\", \"jpeg\", \"bmp\"]):\n",
    "    out = [] \n",
    "    for r, d, f in os.walk(rootdir):\n",
    "        for file in f:\n",
    "            if (file.split('.')[1] in exts)  and  (must_contain in os.path.join(r, file)):\n",
    "                out.append(os.path.join(r, file))\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_list(path, must_contain=''):\n",
    "    if \".pickle\" in path:\n",
    "        with open(path, 'rb') as f:\n",
    "            image_list = pickle.load(f)\n",
    "        image_list = [ item for item in image_list if must_contain in item   ]\n",
    "    else:\n",
    "        image_list = recursively_read(path, must_contain)\n",
    "    return image_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RealFakeDataset(Dataset):\n",
    "    def __init__(self,  real_path, \n",
    "                        fake_path, \n",
    "                        data_mode, \n",
    "                        max_sample,\n",
    "                        arch,\n",
    "                        jpeg_quality=None,\n",
    "                        gaussian_sigma=None):\n",
    "\n",
    "        assert data_mode in [\"wang2020\", \"ours\"]\n",
    "        self.jpeg_quality = jpeg_quality\n",
    "        self.gaussian_sigma = gaussian_sigma\n",
    "        \n",
    "        # = = = = = = data path = = = = = = = = = # \n",
    "        if type(real_path) == str and type(fake_path) == str:\n",
    "            real_list, fake_list = self.read_path(real_path, fake_path, data_mode, max_sample)\n",
    "        else:\n",
    "            real_list = []\n",
    "            fake_list = []\n",
    "            for real_p, fake_p in zip(real_path, fake_path):\n",
    "                real_l, fake_l = self.read_path(real_p, fake_p, data_mode, max_sample)\n",
    "                real_list += real_l\n",
    "                fake_list += fake_l\n",
    "\n",
    "        self.total_list = real_list + fake_list\n",
    "\n",
    "\n",
    "        # = = = = = =  label = = = = = = = = = # \n",
    "\n",
    "        self.labels_dict = {}\n",
    "        for i in real_list:\n",
    "            self.labels_dict[i] = 0\n",
    "        for i in fake_list:\n",
    "            self.labels_dict[i] = 1\n",
    "\n",
    "        stat_from = \"imagenet\" if arch.lower().startswith(\"imagenet\") else \"clip\"\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize( mean=MEAN[stat_from], std=STD[stat_from] ),\n",
    "        ])\n",
    "\n",
    "\n",
    "    def read_path(self, real_path, fake_path, data_mode, max_sample):\n",
    "\n",
    "        if data_mode == 'wang2020':\n",
    "            real_list = get_list(real_path, must_contain='0_real')\n",
    "            fake_list = get_list(fake_path, must_contain='1_fake')\n",
    "        else:\n",
    "            real_list = get_list(real_path)\n",
    "            fake_list = get_list(fake_path)\n",
    "\n",
    "\n",
    "        if max_sample is not None:\n",
    "            if (max_sample > len(real_list)) or (max_sample > len(fake_list)):\n",
    "                max_sample = 100\n",
    "                print(\"not enough images, max_sample falling to 100\")\n",
    "            random.shuffle(real_list)\n",
    "            random.shuffle(fake_list)\n",
    "            real_list = real_list[0:max_sample]\n",
    "            fake_list = fake_list[0:max_sample]\n",
    "\n",
    "        assert len(real_list) == len(fake_list)  \n",
    "\n",
    "        return real_list, fake_list\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.total_list[idx]\n",
    "\n",
    "        label = self.labels_dict[img_path]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.gaussian_sigma is not None:\n",
    "            img = gaussian_blur(img, self.gaussian_sigma) \n",
    "        if self.jpeg_quality is not None:\n",
    "            img = png2jpg(img, self.jpeg_quality)\n",
    "\n",
    "        img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def infer():\n",
    "    class OPT: pass\n",
    "    opt = OPT()\n",
    "    #/home/vkocheganov/work/research_projects/dollar/FAKE_IMAGES/CNN\n",
    "\n",
    "    opt.real_path = None\n",
    "    opt.fake_path = None\n",
    "    opt.data_mode = None\n",
    "    # opt.real_path = f'{base_fold}/FAKE_IMAGES/CNN'\n",
    "    # opt.fake_path = f'{base_fold}/FAKE_IMAGES/CNN'\n",
    "    # opt.data_mode = 'wang2020'\n",
    "    opt.max_sample = 1000\n",
    "    opt.arch = 'CLIP:ViT-L/14' #'res50'\n",
    "    opt.ckpt =  f'{base_fold}/UniversalFakeDetect/checkpoints/clip_vitl14/best'\n",
    "    #f'{base_fold}/UniversalFakeDetect/pretrained_weights/fc_weights.pth' # f\"{base_fold}/UniversalFakeDetect/pretrained_weights/fc_weights.pth\" \n",
    "    opt.result_folder = f'{base_fold}/UniversalFakeDetect/clip_vitl14_1' # 'result' # \n",
    "    opt.batch_size = 32\n",
    "    opt.jpeg_quality = None\n",
    "    opt.gaussian_sigma = None\n",
    "# --arch=CLIP:ViT-L/14   --ckpt=pretrained_weights/fc_weights.pth   --result_folder=clip_vitl14\n",
    "\n",
    "    # if os.path.exists(opt.result_folder):\n",
    "    #     shutil.rmtree(opt.result_folder)\n",
    "    # os.makedirs(opt.result_folder)\n",
    "\n",
    "    model = get_model(opt.arch)\n",
    "    state_dict = torch.load(opt.ckpt, map_location='cpu')\n",
    "    print(state_dict['model'].keys())\n",
    "    state_dict = {\n",
    "        'weight' : state_dict['model']['fc.weight'],\n",
    "        'bias' : state_dict['model']['fc.bias'],\n",
    "                  }\n",
    "    model.fc.load_state_dict(state_dict)\n",
    "    print (\"Model loaded..\")\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "\n",
    "    if (opt.real_path == None) or (opt.fake_path == None) or (opt.data_mode == None):\n",
    "        dataset_paths = DATASET_PATHS\n",
    "    else:\n",
    "        dataset_paths = [ dict(real_path=opt.real_path, fake_path=opt.fake_path, data_mode=opt.data_mode) ]\n",
    "\n",
    "    print(f\"vic dataset_paths = {dataset_paths}\")\n",
    "\n",
    "    for dataset_path in (dataset_paths):\n",
    "        set_seed()\n",
    "        print(f\"processing {dataset_path['key']}\")\n",
    "        dataset = RealFakeDataset(  dataset_path['real_path'], \n",
    "                                    dataset_path['fake_path'], \n",
    "                                    dataset_path['data_mode'], \n",
    "                                    opt.max_sample, \n",
    "                                    opt.arch,\n",
    "                                    jpeg_quality=opt.jpeg_quality, \n",
    "                                    gaussian_sigma=opt.gaussian_sigma,\n",
    "                                    )\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, shuffle=False, num_workers=4)\n",
    "        ap, r_acc0, f_acc0, acc0, r_acc1, f_acc1, acc1, best_thres = validate(model, loader, find_thres=True)\n",
    "\n",
    "        with open( os.path.join(opt.result_folder,'ap.txt'), 'a') as f:\n",
    "            f.write(dataset_path['key']+': ' + str(round(ap*100, 2))+'\\n' )\n",
    "\n",
    "        with open( os.path.join(opt.result_folder,'acc0.txt'), 'a') as f:\n",
    "            f.write(dataset_path['key']+': ' + str(round(r_acc0*100, 2))+'  '+str(round(f_acc0*100, 2))+'  '+str(round(acc0*100, 2))+'\\n' )\n",
    "    return ap, r_acc0, f_acc0, acc0, r_acc1, f_acc1, acc1, best_thres\n",
    "\n",
    "ap, r_acc0, f_acc0, acc0, r_acc1, f_acc1, acc1, best_thres = infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.9090706213211609),\n",
       " 0.88,\n",
       " 0.73,\n",
       " 0.805,\n",
       " 0.8,\n",
       " 0.87,\n",
       " 0.835,\n",
       " np.float64(0.44955483078956604))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap, r_acc0, f_acc0, acc0, r_acc1, f_acc1, acc1, best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.8530035162882252),\n",
       " 0.84,\n",
       " 0.66,\n",
       " 0.75,\n",
       " 0.92,\n",
       " 0.62,\n",
       " 0.77,\n",
       " np.float64(0.5152357220649719))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap, r_acc0, f_acc0, acc0, r_acc1, f_acc1, acc1, best_thres"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakedet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
